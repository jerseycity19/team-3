Notes for Scholars at risk:
  international network of institutions and individuals
  mission is to protect scholars and promote academic freedom

 Scholars are scholars that are threatened for their work
  assassination attempts
  prosecution
  travel restrictions
  govt and non-govt

What they do:
  protection:
    assist at-risk scholars by connecting them w academic oppportunities
  advocacy:
    monitor and report attacks on education
  learning:
    bring ppl together to understand what academic freedom is

Scale is huge:
  >500 institutions
  >5,000 requests
  >700 caseloads
  only tip of iceberg

Challenge:
  most people seeing these attacks "self-censor"
    self-censoring is when you don't say stuff because you know you'll get persecuted for it
  when you see others getting persecuted, why would YOU do the same?
  self-censorship warps knowledge
  scope of self-censorship is MASSIVE
    but if we can expose the scope of self-censorship, we can combat it
      the issue is that it's rly hard and requires a lot of resources

Statment:
  Help us develop a digital vehicle that'll help get a better idea of what self-censorhip is, how it manifests, and who it affects
  Security challenges:
    ppl are scared to report for fear of being found out.
    how to protect data so it doesn't get into bad hands
  generate a large response rate:
    incentivize?
  must be FREE for users/reporters, and cheap for SAR
  multilingual interface/data capture
  simple, attractive visualization of data for non-tech people



NOTES:
  they don't need help with the form that gets help to scholars that need it
  they want help with just figuring out the problem of self-censorship and seeing where it's happening/who it's affecting

Where do we put the form?
  on the website on "get involved" sections
  on the website homepage
  at conferences
    also helps with the security aspect (harder to fake submit)
  my ideas:
    maybe on researchgate?


Ideas:
  webscrape to parse through news data automatically (maybe use NLP techniques)
		could also run NLP algos on things like user comments in the form
			possibly use sentimental analysis?
    then use that data to isolate patterns that help identify fake/real submissions
  we can also use the gathered data to isolate similar patterns to identify fake/real submissions
  use probabilities of fake/real data, conditioning on the web-parsing and previous data to weigh submissions by likeliness     that they're real

=======

  create a global "academic freedom index"?
    assess different countries based off web-parsing
    also based off experts' advice
    account for the data collected by our project
    use this to rate the country on degree of self-censorship
      help regional experts with what they do



  security:
    make sure data isn't faked
    also make sure people can't steal the data

